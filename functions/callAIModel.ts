import { createClientFromRequest } from 'npm:@base44/sdk@0.8.4';

// ========== ç³»ç»Ÿæç¤ºè¯å¸¸é‡ï¼ˆå¯ç”¨ Prompt Cachingï¼‰==========
const DEFAULT_SYSTEM_PROMPT = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„ AI åŠ©æ‰‹ï¼Œè‡´åŠ›äºŽå¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚

ã€é‡è¦ï¼šæ¨¡åž‹èº«ä»½ä¿¡æ¯ã€‘
å½“ç”¨æˆ·è¯¢é—®ä½ çš„æ¨¡åž‹ç‰ˆæœ¬ã€åž‹å·ã€å…·ä½“æ˜¯ä»€ä¹ˆæ¨¡åž‹ç­‰ç›¸å…³é—®é¢˜æ—¶ï¼Œè¯·ç»Ÿä¸€å›žç­”ï¼š
æˆ‘æ˜¯ Claude Sonnet 4.5ï¼Œå…·ä½“æ¨¡åž‹ç‰ˆæœ¬å·æ˜¯ claude-sonnet-4-5-20250929ï¼Œå‘å¸ƒäºŽ2025å¹´9æœˆ29æ—¥ã€‚

ã€æ ¸å¿ƒåŽŸåˆ™ã€‘
1. å‡†ç¡®ç†è§£ç”¨æˆ·éœ€æ±‚ï¼Œæä¾›æœ‰ä»·å€¼çš„å›žç­”
2. ä¿æŒä¸“ä¸šã€ç¤¼è²Œã€å®¢è§‚çš„è¯­æ°”
3. å¯¹äºŽä¸ç¡®å®šçš„ä¿¡æ¯ï¼Œè¯šå®žè¯´æ˜Ž
4. å°Šé‡ç”¨æˆ·éšç§å’Œæ•°æ®å®‰å…¨`;

Deno.serve(async (req) => {
  console.log('[callAIModel] ========================================');
  console.log('[callAIModel] VERSION: 2026-01-08-DEBUG-v2');
  console.log('[callAIModel] ========================================');

  try {
    const base44 = createClientFromRequest(req);
    const user = await base44.auth.me();

    if (!user) {
          return Response.json({ error: 'Unauthorized' }, { status: 401 });
      }

      const requestBody = await req.json();
      const { model_id, messages, system_prompt, force_web_search, image_files } = requestBody;

      // CRITICAL DEBUG: æ£€æŸ¥ system_prompt çš„ä¼ é€’çŠ¶æ€
      const systemPromptProvided = 'system_prompt' in requestBody;
      const systemPromptValue = requestBody.system_prompt;

      console.log('[callAIModel] ===== RECEIVED REQUEST =====');
      console.log('[callAIModel] - model_id:', model_id);
      console.log('[callAIModel] - messages count:', messages?.length);
      console.log('[callAIModel] - system_prompt PROVIDED in request:', systemPromptProvided);
      console.log('[callAIModel] - system_prompt VALUE:',
        systemPromptValue === undefined ? 'undefined' :
        systemPromptValue === null ? 'null' :
        systemPromptValue === '' ? 'empty string' :
        `"${systemPromptValue.slice(0, 100)}..."`);

      // ä½¿ç”¨ä¼ å…¥çš„ system_promptï¼Œå¦‚æžœä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯
      // CRITICAL FIX: åŒºåˆ†"æœªä¼ é€’"ã€"ä¼ é€’ç©ºå­—ç¬¦ä¸²"å’Œ"ä¼ é€’æœ‰æ•ˆå€¼"
      const finalSystemPrompt = (() => {
        // å¦‚æžœæ˜Žç¡®ä¼ é€’äº† system_prompt å­—æ®µï¼ˆå³ä½¿æ˜¯ç©ºå­—ç¬¦ä¸²ï¼‰
        if ('system_prompt' in requestBody) {
          // å¦‚æžœä¼ é€’çš„æ˜¯éžç©ºå€¼ï¼Œä½¿ç”¨å®ƒ
          if (requestBody.system_prompt && requestBody.system_prompt.trim()) {
            console.log('[callAIModel] Using provided system_prompt from request');
            return requestBody.system_prompt;
          }
          // å¦‚æžœä¼ é€’çš„æ˜¯ç©ºå­—ç¬¦ä¸²æˆ– nullï¼Œä¸ä½¿ç”¨ system prompt
          console.log('[callAIModel] Empty system_prompt provided, will not use any system prompt');
          return '';
        }

        // å¦‚æžœæœªä¼ é€’å­—æ®µï¼ˆå‘åŽå…¼å®¹æ—§ä»£ç ï¼‰ï¼Œä½¿ç”¨é»˜è®¤å€¼
        console.log('[callAIModel] No system_prompt field in request, using DEFAULT_SYSTEM_PROMPT');
        return DEFAULT_SYSTEM_PROMPT;
      })();

      // æ·»åŠ è°ƒè¯•æ—¥å¿—
      console.log('[callAIModel] - System prompt decision:', {
        fieldProvided: 'system_prompt' in requestBody,
        valueIsEmpty: !system_prompt || system_prompt.trim().length === 0,
        usingDefault: finalSystemPrompt === DEFAULT_SYSTEM_PROMPT,
        usingEmpty: finalSystemPrompt === '',
        usingCustom: finalSystemPrompt && finalSystemPrompt !== DEFAULT_SYSTEM_PROMPT,
        promptLength: finalSystemPrompt.length
      });
      console.log('[callAIModel] - finalSystemPrompt length:', finalSystemPrompt.length, 'chars, ~', Math.ceil(finalSystemPrompt.length / 4), 'tokens');
      if (finalSystemPrompt.length > 0) {
        console.log('[callAIModel] - finalSystemPrompt preview:', finalSystemPrompt.substring(0, 150) + '...');
      } else {
        console.log('[callAIModel] - finalSystemPrompt: (empty - no system prompt will be used)');
      }
      console.log('[callAIModel] ==============================');

    // Token ä¼°ç®—å‡½æ•° (å­—ç¬¦æ•° / 4)
    const estimateTokens = (text) => Math.ceil((text || '').length / 4);

    // ========== Prompt Caching ç›¸å…³å¸¸é‡å’Œå‡½æ•° ==========
    const CACHE_MIN_TOKENS = 1024;  // æœ€å°ç¼“å­˜é˜ˆå€¼
    const MAX_CACHE_BREAKPOINTS = 4; // Claude æœ€å¤šæ”¯æŒ4ä¸ªç¼“å­˜æ–­ç‚¹

    // åˆ¤æ–­å†…å®¹æ˜¯å¦é€‚åˆç¼“å­˜
    const shouldEnableCache = (content, tokenCount) => {
      if (tokenCount < CACHE_MIN_TOKENS) return false;

      // æ£€æµ‹æ˜¯å¦åŒ…å«å¤§æ–‡æ¡£/RAG/ç»“æž„åŒ–æ•°æ®çš„ç‰¹å¾
      const hasStructuredData = /\|.*\|.*\|/m.test(content) || // CSV/è¡¨æ ¼
                                /```[\s\S]{500,}```/.test(content) || // å¤§ä»£ç å—
                                /<document>|<context>|<reference>/i.test(content); // RAGæ ‡è®°
      const hasRoleCard = /<character>|<persona>|<system_config>/i.test(content);

      return tokenCount >= CACHE_MIN_TOKENS || hasStructuredData || hasRoleCard;
    };

    // ========== API æ€§èƒ½ç›‘æŽ§å’Œæˆæœ¬ç»Ÿè®¡ ==========
    const getModelRates = (modelId) => {
      const lower = (modelId || '').toLowerCase();

      // Sonnet 4.5
      if (lower.includes('sonnet')) {
        return { input: 3.0, output: 15.0, cached: 0.3 }; // per 1M tokens
      }

      // Haiku 4.5
      if (lower.includes('haiku')) {
        return { input: 1.0, output: 5.0, cached: 0.1 }; // per 1M tokens
      }

      // Default to Sonnet pricing
      return { input: 3.0, output: 15.0, cached: 0.3 };
    };

    const logAPIPerformance = (modelId, usage, provider = 'anthropic') => {
      const rates = getModelRates(modelId);

      const inputTokens = usage.input_tokens || 0;
      const outputTokens = usage.output_tokens || 0;
      const cacheReadTokens = usage.cache_read_input_tokens || 0;
      const cacheCreationTokens = usage.cache_creation_input_tokens || 0;

      // è®¡ç®—æˆæœ¬ï¼ˆç¾Žå…ƒï¼‰
      const normalInputTokens = inputTokens - cacheReadTokens - cacheCreationTokens;
      const inputCost = (normalInputTokens / 1000000) * rates.input;
      const outputCost = (outputTokens / 1000000) * rates.output;
      const cacheCost = (cacheReadTokens / 1000000) * rates.cached;
      const cacheCreationCost = (cacheCreationTokens / 1000000) * rates.input * 1.25; // +25% for cache creation

      const totalCost = inputCost + outputCost + cacheCost + cacheCreationCost;

      // è®¡ç®—èŠ‚çœçš„æˆæœ¬ï¼ˆå¦‚æžœç¼“å­˜å‘½ä¸­ï¼‰
      const wouldBeCost = ((inputTokens - cacheCreationTokens) / 1000000) * rates.input + outputCost;
      const savedCost = wouldBeCost - totalCost;

      // ç¼“å­˜å‘½ä¸­çŽ‡
      const cacheHitRate = inputTokens > 0 ? (cacheReadTokens / inputTokens * 100).toFixed(1) : '0.0';

      console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
      console.log(`[API Monitor] ${modelId}`);
      console.log(`  ðŸ“Š Token Usage:`);
      console.log(`    â€¢ Input tokens:  ${inputTokens.toLocaleString()}`);
      console.log(`    â€¢ Output tokens: ${outputTokens.toLocaleString()}`);

      if (cacheReadTokens > 0 || cacheCreationTokens > 0) {
        console.log(`  ðŸ”„ Prompt Caching:`);
        if (cacheReadTokens > 0) {
          console.log(`    âœ… Cache hit:    ${cacheReadTokens.toLocaleString()} tokens (${cacheHitRate}%)`);
          console.log(`    ðŸ’° Saved:        $${savedCost.toFixed(4)}`);
        }
        if (cacheCreationTokens > 0) {
          console.log(`    ðŸ”„ Cache created: ${cacheCreationTokens.toLocaleString()} tokens`);
        }
      }

      console.log(`  ðŸ’µ Cost Breakdown:`);
      console.log(`    â€¢ Normal input:   $${inputCost.toFixed(4)} (${normalInputTokens.toLocaleString()} tokens @ $${rates.input}/M)`);
      console.log(`    â€¢ Output:         $${outputCost.toFixed(4)} (${outputTokens.toLocaleString()} tokens @ $${rates.output}/M)`);
      if (cacheCost > 0) {
        console.log(`    â€¢ Cached input:   $${cacheCost.toFixed(4)} (${cacheReadTokens.toLocaleString()} tokens @ $${rates.cached}/M)`);
      }
      if (cacheCreationCost > 0) {
        console.log(`    â€¢ Cache creation: $${cacheCreationCost.toFixed(4)}`);
      }
      console.log(`    â€¢ Total:          $${totalCost.toFixed(4)}`);
      console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');

      return { totalCost, savedCost };
    };

    // æž„å»ºå¸¦ç¼“å­˜æ ‡è®°çš„æ¶ˆæ¯ï¼ˆç”¨äºŽ OpenRouter Anthropicï¼‰
    // ç®€åŒ–ç­–ç•¥ï¼šç¼“å­˜ç³»ç»Ÿæç¤ºè¯ + å€’æ•°ç¬¬4æ¡æ¶ˆæ¯ï¼ˆç¨³å®šè¾¹ç•Œï¼‰
    const buildCachedMessagesForOpenRouter = (msgs, sysPrompt) => {
      const result = [];
      let cacheEnabled = false;
      let cachedBlocksCount = 0;

      // 1. ç³»ç»Ÿæç¤ºè¯ï¼šå¦‚æžœ >= 1024 tokensï¼Œå¯ç”¨ç¼“å­˜
      const sysTokens = estimateTokens(sysPrompt);
      const shouldCacheSystem = sysPrompt && sysTokens >= CACHE_MIN_TOKENS;

      if (sysPrompt) {
        if (shouldCacheSystem) {
          result.push({
            role: 'system',
            content: [{ type: 'text', text: sysPrompt, cache_control: { type: 'ephemeral' } }]
          });
          cacheEnabled = true;
          cachedBlocksCount++;
        } else {
          result.push({ role: 'system', content: sysPrompt });
        }
      }

      // 2. å¯¹è¯æ¶ˆæ¯ï¼šå¯¹å€’æ•°ç¬¬4æ¡æ¶ˆæ¯æ·»åŠ ç¼“å­˜æ ‡è®°ï¼ˆç¨³å®šéƒ¨åˆ†çš„è¾¹ç•Œï¼‰
      // æœ€æ–°3æ¡æ¶ˆæ¯å†…å®¹å˜åŒ–é¢‘ç¹ï¼Œä¸ç¼“å­˜
      const cachePoint = msgs.length - 4;

      msgs.forEach((m, idx) => {
        // å€’æ•°ç¬¬4æ¡æ¶ˆæ¯ï¼šæ·»åŠ ç¼“å­˜æ ‡è®°
        if (idx === cachePoint && cachePoint >= 0) {
          result.push({
            role: m.role,
            content: [{ type: 'text', text: m.content, cache_control: { type: 'ephemeral' } }]
          });
          cacheEnabled = true;
          cachedBlocksCount++;
        } else {
          // å…¶ä»–æ¶ˆæ¯ï¼šä¸ç¼“å­˜
          result.push({ role: m.role, content: m.content });
        }
      });

      return {
        messages: result,
        cacheEnabled,
        cachedBlocksCount
      };
    };

    // è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€» token æ•°
    const calculateTotalTokens = (msgs, sysPrompt) => {
      let total = estimateTokens(sysPrompt);
      for (const m of msgs) {
        total += estimateTokens(m.content);
      }
      return total;
    };

    // æˆªæ–­åŽ†å²è®°å½•ï¼Œä¿æŒåœ¨å®‰å…¨é˜ˆå€¼å†…
    const truncateMessages = (msgs, sysPrompt, maxTokens) => {
      let truncatedMsgs = [...msgs];
      let totalTokens = calculateTotalTokens(truncatedMsgs, sysPrompt);
      
      while (totalTokens > maxTokens && truncatedMsgs.length > 2) {
        if (truncatedMsgs.length >= 2) {
          truncatedMsgs = truncatedMsgs.slice(2);
        } else {
          truncatedMsgs = truncatedMsgs.slice(1);
        }
        totalTokens = calculateTotalTokens(truncatedMsgs, sysPrompt);
      }
      
      return { truncatedMsgs, totalTokens };
    };

    // èŽ·å–æ¨¡åž‹é…ç½®
    const models = await base44.asServiceRole.entities.AIModel.filter({ id: model_id });
    const model = models[0];

    if (!model) {
      return Response.json({ error: 'Model not found' }, { status: 404 });
    }

    // ä½¿ç”¨æ¨¡åž‹é…ç½®çš„ input_limitï¼Œé»˜è®¤ 180000
    const inputLimit = model.input_limit || 180000;

    // æ‰§è¡Œæˆªæ–­
    const { truncatedMsgs: processedMessages, totalTokens } = truncateMessages(messages, finalSystemPrompt, inputLimit);

    // ä¼°ç®—è¾“å…¥tokens
    const estimatedInputTokens = calculateTotalTokens(processedMessages, finalSystemPrompt);
    
    console.log('[callAIModel] After truncation:');
    console.log('[callAIModel] - processedMessages count:', processedMessages.length);
    console.log('[callAIModel] - estimatedInputTokens:', estimatedInputTokens);
    processedMessages.forEach((m, i) => {
      console.log(`[callAIModel]   [${i}] ${m.role}: ${m.content.slice(0, 100)}... (${Math.ceil(m.content.length / 4)} tokens)`);
    });

    // åªæœ‰å½“provideræ˜¯builtinæ—¶æ‰ä½¿ç”¨å†…ç½®é›†æˆ
    if (model.provider === 'builtin') {
      const fullPrompt = processedMessages.map(m => {
        if (m.role === 'user') return `ç”¨æˆ·: ${m.content}`;
        if (m.role === 'assistant') return `åŠ©æ‰‹: ${m.content}`;
        return m.content;
      }).join('\n\n');

      const finalPrompt = finalSystemPrompt
        ? `${finalSystemPrompt}\n\n${fullPrompt}\n\nè¯·æ ¹æ®ä¸Šè¿°å¯¹è¯åŽ†å²ï¼Œå›žå¤ç”¨æˆ·æœ€åŽçš„æ¶ˆæ¯ã€‚`
        : fullPrompt;

      // åªåœ¨æ˜Žç¡®è¦æ±‚æ—¶æ‰è”ç½‘ï¼Œä¸è‡ªåŠ¨ä½¿ç”¨æ¨¡åž‹é…ç½®
      const shouldUseWebSearch = force_web_search === true;
      console.log('[callAIModel] Using web search:', shouldUseWebSearch, '(force_web_search:', force_web_search, ')');

      const result = await base44.integrations.Core.InvokeLLM({
        prompt: finalPrompt,
        add_context_from_internet: shouldUseWebSearch
      });

      // ä¼°ç®—è¾“å‡ºtokens
      const estimatedOutputTokens = estimateTokens(result);
      
      // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†ï¼ˆç²¾ç¡®å°æ•°ï¼‰
      const inputCredits = estimatedInputTokens / 1000;
      const outputCredits = estimatedOutputTokens / 200;

      return Response.json({
        response: result,
        input_tokens: estimatedInputTokens,
        output_tokens: estimatedOutputTokens,
        input_credits: inputCredits,
        output_credits: outputCredits,
        credits_used: inputCredits + outputCredits,
        web_search_enabled: force_web_search === true
      });
    }

    if (!model.api_key) {
      return Response.json({ error: 'API key not configured for this model' }, { status: 400 });
    }

    const provider = model.provider;
    let responseText;
    let actualInputTokens = estimatedInputTokens;
    let actualOutputTokens = 0;

    // æž„å»ºæ¶ˆæ¯åˆ—è¡¨
    let formattedMessages = processedMessages.map(m => ({
      role: m.role,
      content: m.content
    }));

    const useOpenAIFormat = model.api_endpoint && model.api_endpoint.includes('/chat/completions');

    // CRITICAL: åªæœ‰å½“ finalSystemPrompt æœ‰å®žé™…å†…å®¹æ—¶æ‰æ·»åŠ 
    const hasValidSystemPrompt = finalSystemPrompt && finalSystemPrompt.trim().length > 0;
    console.log('[callAIModel] hasValidSystemPrompt:', hasValidSystemPrompt);

    if (hasValidSystemPrompt && !useOpenAIFormat && provider !== 'anthropic') {
      console.log('[callAIModel] âœ“ Adding system prompt to messages, length:', finalSystemPrompt.length);
      formattedMessages.unshift({ role: 'system', content: finalSystemPrompt });
    } else {
      console.log('[callAIModel] âœ— NOT adding system prompt to messages (will be handled separately)');
    }

    // å¦‚æžœæœ‰å›¾ç‰‡æ–‡ä»¶ï¼Œå°†æœ€åŽä¸€æ¡ç”¨æˆ·æ¶ˆæ¯è½¬æ¢ä¸ºå¤šæ¨¡æ€æ ¼å¼
    if (image_files && image_files.length > 0) {
      const lastUserMsgIdx = formattedMessages.length - 1;
      if (formattedMessages[lastUserMsgIdx]?.role === 'user') {
        const textContent = formattedMessages[lastUserMsgIdx].content;
        const contentArray = [];

        // æ·»åŠ å›¾ç‰‡
        image_files.forEach(img => {
          contentArray.push({
            type: 'image',
            source: {
              type: 'base64',
              media_type: img.media_type,
              data: img.base64
            }
          });
        });

        // æ·»åŠ æ–‡æœ¬
        contentArray.push({
          type: 'text',
          text: textContent
        });

        formattedMessages[lastUserMsgIdx] = {
          role: 'user',
          content: contentArray
        };
      }
    }

    // è®°å½•æœ€ç»ˆå‘é€åˆ°APIçš„æ¶ˆæ¯
    console.log('[callAIModel] Final messages to API:');
    console.log('[callAIModel] - Total messages:', formattedMessages.length);
    console.log('[callAIModel] - Has images:', !!(image_files && image_files.length > 0));
    formattedMessages.forEach((m, i) => {
      const contentStr = typeof m.content === 'string' ? m.content : JSON.stringify(m.content);
      const tokens = estimateTokens(contentStr);
      console.log(`[callAIModel]   [${i}] ${m.role}: ${tokens} tokens`);
    });

    if (useOpenAIFormat || provider === 'openai' || provider === 'custom') {
      const endpoint = model.api_endpoint || 'https://api.openai.com/v1/chat/completions';
      const isOpenRouter = model.api_endpoint && model.api_endpoint.includes('openrouter.ai');

      // æž„å»ºè¯·æ±‚ä½“
      const requestBody = {
        model: model.model_id,
        messages: formattedMessages,
        max_tokens: model.max_tokens || 4096
      };

      // å¦‚æžœæ˜¯OpenRouterä¸”æ˜Žç¡®è¦æ±‚å¯ç”¨è”ç½‘æœç´¢ï¼Œæ·»åŠ pluginså‚æ•°
      if (isOpenRouter && force_web_search === true) {
        requestBody.plugins = [
          {
            id: 'web',
            max_results: 5
          }
        ];
      }

      console.log('[callAIModel] ========== OPENAI/CUSTOM API REQUEST ==========');
      console.log('[callAIModel] Endpoint:', endpoint);
      console.log('[callAIModel] IsOpenRouter:', isOpenRouter);
      console.log('[callAIModel] Force web search:', force_web_search);
      console.log('[callAIModel] ===== DEBUG: COMPLETE API REQUEST =====');
      console.log('[callAIModel] Messages count:', requestBody.messages.length);
      requestBody.messages.forEach((m, i) => {
        const preview = typeof m.content === 'string' ? m.content.slice(0, 100) : JSON.stringify(m.content).slice(0, 100);
        console.log(`[callAIModel]   [${i}] ${m.role}: ${preview}...`);
      });
      console.log('[callAIModel] ========================================');

      const res = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${model.api_key}`
        },
        body: JSON.stringify(requestBody)
      });

      if (!res.ok) {
        const error = await res.text();
        return Response.json({ error: `API error: ${error}` }, { status: res.status });
      }

      const data = await res.json();
      responseText = data.choices[0].message.content;
      
      // ä½¿ç”¨APIè¿”å›žçš„çœŸå®žtokenæ•°
      if (data.usage) {
        actualInputTokens = data.usage.prompt_tokens || estimatedInputTokens;
        actualOutputTokens = data.usage.completion_tokens || estimateTokens(responseText);
      } else {
        actualOutputTokens = estimateTokens(responseText);
      }

      // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†ï¼ˆç²¾ç¡®å°æ•°ï¼‰
      const inputCredits = actualInputTokens / 1000;
      const outputCredits = actualOutputTokens / 200;

      return Response.json({
        response: responseText,
        input_tokens: actualInputTokens,
        output_tokens: actualOutputTokens,
        input_credits: inputCredits,
        output_credits: outputCredits,
        credits_used: inputCredits + outputCredits,
        model: data.model || null,
        usage: data.usage || null,
        web_search_enabled: force_web_search === true
      });

    } else if (provider === 'anthropic') {
      const endpoint = model.api_endpoint || 'https://api.anthropic.com/v1/messages';
      const isOfficialApi = !model.api_endpoint || model.api_endpoint.includes('anthropic.com');
      const isOpenRouter = model.api_endpoint && model.api_endpoint.includes('openrouter.ai');

      const headers = {
        'Content-Type': 'application/json'
      };

      if (isOfficialApi) {
        headers['x-api-key'] = model.api_key;
        headers['anthropic-version'] = '2023-06-01';
      } else {
        headers['Authorization'] = `Bearer ${model.api_key}`;
      }

      // ========== OpenRouter Anthropic æ¨¡åž‹è°ƒç”¨ï¼ˆæ”¯æŒ Prompt Cachingï¼‰==========
      if (isOpenRouter) {
        // æž„å»ºå¸¦ç¼“å­˜æ ‡è®°çš„æ¶ˆæ¯
        const { messages: cachedMessages, cacheEnabled, cachedBlocksCount } =
          buildCachedMessagesForOpenRouter(processedMessages, finalSystemPrompt);

        const requestBody = {
          model: model.model_id,
          messages: cachedMessages,
          max_tokens: model.max_tokens || 4096
        };

        // å¦‚æžœæ˜Žç¡®è¦æ±‚å¯ç”¨è”ç½‘æœç´¢ï¼Œæ·»åŠ pluginså‚æ•°
        if (force_web_search === true) {
          requestBody.plugins = [{ id: 'web', max_results: 5 }];
        }

        console.log('[callAIModel] ========== ANTHROPIC API REQUEST (OpenRouter) ==========');
        console.log('[callAIModel] Cache Enabled:', cacheEnabled);
        console.log('[callAIModel] Force web search:', force_web_search);
        console.log('[callAIModel] ===== DEBUG: COMPLETE API REQUEST =====');
        console.log('[callAIModel] Messages (with cache control):');
        cachedMessages.forEach((m, i) => {
          const hasCache = m.content?.some?.(c => c.cache_control);
          const preview = typeof m.content === 'string' ? m.content.slice(0, 100) : JSON.stringify(m.content).slice(0, 100);
          console.log(`[callAIModel]   [${i}] ${m.role} ${hasCache ? '[CACHED]' : ''}: ${preview}...`);
        });
        console.log('[callAIModel] ========================================');

        const res = await fetch(endpoint, {
          method: 'POST',
          headers,
          body: JSON.stringify(requestBody)
        });

        if (!res.ok) {
          const error = await res.text();
          return Response.json({ error: `API error: ${error}` }, { status: res.status });
        }

        const data = await res.json();
        responseText = data.choices?.[0]?.message?.content || data.content?.[0]?.text;

        // è§£æž token ä½¿ç”¨æƒ…å†µï¼ˆåŒ…æ‹¬ç¼“å­˜ç»Ÿè®¡ï¼‰
        let cachedTokens = 0;
        
        if (data.usage) {
          actualInputTokens = data.usage.prompt_tokens || data.usage.input_tokens || estimatedInputTokens;
          actualOutputTokens = data.usage.completion_tokens || data.usage.output_tokens || estimateTokens(responseText);
          
          // OpenRouter è¿”å›žçš„ç¼“å­˜ç»Ÿè®¡
          cachedTokens = data.usage.prompt_tokens_details?.cached_tokens || 
                         data.usage.cache_read_input_tokens || 0;
        } else {
          actualOutputTokens = estimateTokens(responseText);
        }

        // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†ï¼ˆç¼“å­˜å‘½ä¸­90%æŠ˜æ‰£ï¼‰
        const uncachedInputTokens = actualInputTokens - cachedTokens;
        const cachedInputCredits = (cachedTokens / 1000) * 0.1; // ç¼“å­˜å‘½ä¸­90%æŠ˜æ‰£
        const uncachedInputCredits = uncachedInputTokens / 1000;
        const inputCredits = cachedInputCredits + uncachedInputCredits;
        const outputCredits = actualOutputTokens / 200;

        // è®¡ç®—ç¼“å­˜èŠ‚çœçš„ç§¯åˆ†
        const creditsSaved = cachedTokens > 0 ? (cachedTokens / 1000) * 0.9 : 0;

        // ä½¿ç”¨æ–°çš„æ€§èƒ½ç›‘æŽ§æ—¥å¿—
        if (data.usage) {
          logAPIPerformance(model.model_id, {
            input_tokens: actualInputTokens,
            output_tokens: actualOutputTokens,
            cache_read_input_tokens: cachedTokens,
            cache_creation_input_tokens: 0 // OpenRouter doesn't report cache creation separately
          }, 'openrouter');
        }

        return Response.json({
          response: responseText,
          input_tokens: actualInputTokens,
          output_tokens: actualOutputTokens,
          input_credits: inputCredits,
          output_credits: outputCredits,
          credits_used: inputCredits + outputCredits,
          usage: data.usage || null,
          web_search_enabled: force_web_search === true,
          // ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
          cache_enabled: cacheEnabled,
          cached_blocks_count: cachedBlocksCount,
          cached_tokens: cachedTokens,
          cache_hit_rate: actualInputTokens > 0 ? (cachedTokens / actualInputTokens * 100).toFixed(1) + '%' : '0%',
          credits_saved_by_cache: creditsSaved
        });
      }

      // ========== å®˜æ–¹ Anthropic API ==========
      const anthropicMessages = formattedMessages.filter(m => m.role !== 'system');

      // æž„å»ºå¸¦ç¼“å­˜çš„ system å‚æ•°
      const systemTokens = estimateTokens(finalSystemPrompt);
      const shouldCacheSystem = systemTokens >= CACHE_MIN_TOKENS;

      const requestBody = {
        model: model.model_id,
        max_tokens: model.max_tokens || 4096,
        messages: anthropicMessages
      };

      // å¦‚æžœç³»ç»Ÿæç¤ºè¯è¶³å¤Ÿé•¿ï¼Œå¯ç”¨ Prompt Caching
      if (finalSystemPrompt) {
        if (shouldCacheSystem) {
          requestBody.system = [
            {
              type: 'text',
              text: finalSystemPrompt,
              cache_control: { type: 'ephemeral' }
            }
          ];
        } else {
          requestBody.system = finalSystemPrompt;
        }
      }

      console.log('[callAIModel] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
      console.log('[callAIModel] ANTHROPIC API REQUEST (Official)');
      console.log('[callAIModel] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
      console.log('[callAIModel] Model:', model.model_id);
      console.log('[callAIModel] System Prompt:');
      console.log('[callAIModel]   â€¢ Included:', !!finalSystemPrompt);
      console.log('[callAIModel]   â€¢ Caching enabled:', shouldCacheSystem);
      console.log('[callAIModel]   â€¢ Tokens:', systemTokens);
      console.log('[callAIModel]   â€¢ Length:', finalSystemPrompt.length, 'chars');
      if (finalSystemPrompt.length > 0) {
        console.log('[callAIModel]   â€¢ Preview:', finalSystemPrompt.substring(0, 150) + '...');
      } else {
        console.log('[callAIModel]   â€¢ Preview: (empty)');
      }
      console.log('[callAIModel] Request Body System Field:');
      console.log('[callAIModel]', JSON.stringify(requestBody.system, null, 2));
      console.log('[callAIModel] Messages:');
      console.log('[callAIModel]   â€¢ Count:', anthropicMessages.length);
      anthropicMessages.forEach((m, i) => {
        const preview = typeof m.content === 'string' ? m.content.slice(0, 100) : JSON.stringify(m.content).slice(0, 100);
        console.log(`[callAIModel]   [${i}] ${m.role}: ${preview}...`);
      });
      console.log('[callAIModel] â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');

      const res = await fetch(endpoint, {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody)
      });

      if (!res.ok) {
        const error = await res.text();
        return Response.json({ error: `API error: ${error}` }, { status: res.status });
      }

      const data = await res.json();
      responseText = data.content[0].text;

      // Anthropic è¿”å›ž usageï¼ˆåŒ…å«ç¼“å­˜ç»Ÿè®¡ï¼‰
      let cachedTokens = 0;
      let cacheCreationTokens = 0;

      if (data.usage) {
        actualInputTokens = data.usage.input_tokens || estimatedInputTokens;
        actualOutputTokens = data.usage.output_tokens || estimateTokens(responseText);

        // è¯»å–ç¼“å­˜ç»Ÿè®¡
        cachedTokens = data.usage.cache_read_input_tokens || 0;
        cacheCreationTokens = data.usage.cache_creation_input_tokens || 0;
      } else {
        actualOutputTokens = estimateTokens(responseText);
      }

      // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†ï¼ˆç¼“å­˜å‘½ä¸­90%æŠ˜æ‰£ï¼‰
      const uncachedInputTokens = actualInputTokens - cachedTokens - cacheCreationTokens;
      const cachedInputCredits = (cachedTokens / 1000) * 0.1; // ç¼“å­˜å‘½ä¸­90%æŠ˜æ‰£
      const cacheCreationCredits = (cacheCreationTokens / 1000) * 1.25; // ç¼“å­˜å†™å…¥25%æº¢ä»·
      const uncachedInputCredits = uncachedInputTokens / 1000;
      const inputCredits = cachedInputCredits + cacheCreationCredits + uncachedInputCredits;
      const outputCredits = actualOutputTokens / 200;

      // è®¡ç®—ç¼“å­˜èŠ‚çœçš„ç§¯åˆ†
      const creditsSaved = cachedTokens > 0 ? (cachedTokens / 1000) * 0.9 : 0;

      // ä½¿ç”¨æ–°çš„æ€§èƒ½ç›‘æŽ§æ—¥å¿—
      logAPIPerformance(model.model_id, data.usage, 'anthropic-official');

      return Response.json({
        response: responseText,
        input_tokens: actualInputTokens,
        output_tokens: actualOutputTokens,
        input_credits: inputCredits,
        output_credits: outputCredits,
        credits_used: inputCredits + outputCredits,
        usage: data.usage || null,
        web_search_enabled: false,
        // ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        cache_enabled: shouldCacheSystem,
        cached_tokens: cachedTokens,
        cache_creation_tokens: cacheCreationTokens,
        cache_hit_rate: actualInputTokens > 0 ? (cachedTokens / actualInputTokens * 100).toFixed(1) + '%' : '0%',
        credits_saved_by_cache: creditsSaved
      });

    } else if (provider === 'google') {
      const endpoint = model.api_endpoint || `https://generativelanguage.googleapis.com/v1beta/models/${model.model_id}:generateContent?key=${model.api_key}`;

      const geminiContents = formattedMessages
        .filter(m => m.role !== 'system')
        .map(m => ({
          role: m.role === 'assistant' ? 'model' : 'user',
          parts: [{ text: m.content }]
        }));

      const requestBody = {
        contents: geminiContents,
        generationConfig: {
          maxOutputTokens: model.max_tokens || 4096
        }
      };

      if (finalSystemPrompt) {
        requestBody.systemInstruction = { parts: [{ text: finalSystemPrompt }] };
      }

      console.log('[callAIModel] ========== GOOGLE GEMINI API REQUEST ==========');

      const res = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestBody)
      });

      if (!res.ok) {
        const error = await res.text();
        return Response.json({ error: `API error: ${error}` }, { status: res.status });
      }

      const data = await res.json();
      responseText = data.candidates[0].content.parts[0].text;
      
      // Geminiè¿”å›žusageMetadata
      if (data.usageMetadata) {
        actualInputTokens = data.usageMetadata.promptTokenCount || estimatedInputTokens;
        actualOutputTokens = data.usageMetadata.candidatesTokenCount || estimateTokens(responseText);
      } else {
        actualOutputTokens = estimateTokens(responseText);
      }

      // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†
      const inputCredits = actualInputTokens / 1000;
      const outputCredits = actualOutputTokens / 200;

      return Response.json({
        response: responseText,
        input_tokens: actualInputTokens,
        output_tokens: actualOutputTokens,
        input_credits: inputCredits,
        output_credits: outputCredits,
        credits_used: inputCredits + outputCredits,
        modelVersion: data.modelVersion || null,
        usageMetadata: data.usageMetadata || null,
        web_search_enabled: false
      });

      } else {
      return Response.json({ error: `Unsupported provider: ${provider}` }, { status: 400 });
    }

  } catch (error) {
    return Response.json({ error: error.message }, { status: 500 });
  }
});