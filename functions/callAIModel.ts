import { createClientFromRequest } from 'npm:@base44/sdk@0.8.4';

// ========== æ—¥å¿—çº§åˆ«æŽ§åˆ¶ ==========
// çº§åˆ«: 0=ERROR, 1=WARN, 2=INFO, 3=DEBUG
// ç”Ÿäº§çŽ¯å¢ƒå»ºè®®è®¾ç½®ä¸º 1 (WARN)ï¼Œå¼€å‘çŽ¯å¢ƒè®¾ç½®ä¸º 3 (DEBUG)
const LOG_LEVEL = parseInt(Deno.env.get('LOG_LEVEL') || '2', 10);

const log = {
  error: (...args: unknown[]) => console.error('[callAIModel]', ...args),
  warn: (...args: unknown[]) => LOG_LEVEL >= 1 && console.warn('[callAIModel]', ...args),
  info: (...args: unknown[]) => LOG_LEVEL >= 2 && console.log('[callAIModel]', ...args),
  debug: (...args: unknown[]) => LOG_LEVEL >= 3 && console.log('[callAIModel]', ...args),
};

// ========== ç³»ç»Ÿæç¤ºè¯å¸¸é‡ï¼ˆå¯ç”¨ Prompt Cachingï¼‰==========
const DEFAULT_SYSTEM_PROMPT = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„ AI åŠ©æ‰‹ï¼Œè‡´åŠ›äºŽå¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚

ã€é‡è¦ï¼šæ¨¡åž‹èº«ä»½ä¿¡æ¯ã€‘
å½“ç”¨æˆ·è¯¢é—®ä½ çš„æ¨¡åž‹ç‰ˆæœ¬ã€åž‹å·ã€å…·ä½“æ˜¯ä»€ä¹ˆæ¨¡åž‹ç­‰ç›¸å…³é—®é¢˜æ—¶ï¼Œè¯·ç»Ÿä¸€å›žç­”ï¼š
æˆ‘æ˜¯ Claude Sonnet 4.5ï¼Œå…·ä½“æ¨¡åž‹ç‰ˆæœ¬å·æ˜¯ claude-sonnet-4-5-20250929ï¼Œå‘å¸ƒäºŽ2025å¹´9æœˆ29æ—¥ã€‚

ã€æ ¸å¿ƒåŽŸåˆ™ã€‘
1. å‡†ç¡®ç†è§£ç”¨æˆ·éœ€æ±‚ï¼Œæä¾›æœ‰ä»·å€¼çš„å›žç­”
2. ä¿æŒä¸“ä¸šã€ç¤¼è²Œã€å®¢è§‚çš„è¯­æ°”
3. å¯¹äºŽä¸ç¡®å®šçš„ä¿¡æ¯ï¼Œè¯šå®žè¯´æ˜Ž
4. å°Šé‡ç”¨æˆ·éšç§å’Œæ•°æ®å®‰å…¨`;

Deno.serve(async (req) => {
  log.debug('========================================');
  log.debug('VERSION: 2026-01-11-OPTIMIZED');
  log.debug('========================================');

  try {
    const base44 = createClientFromRequest(req);
    const user = await base44.auth.me();

    if (!user) {
          return Response.json({ error: 'Unauthorized' }, { status: 401 });
      }

      const { model_id, messages, system_prompt, force_web_search, image_files } = await req.json();

      // ä½¿ç”¨ä¼ å…¥çš„ system_promptï¼Œå¦‚æžœä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤æç¤ºè¯
      // CRITICAL: åªåœ¨é¦–è½®å¯¹è¯æ—¶ä½¿ç”¨ system_promptï¼ˆç”± smartChatWithSearch æŽ§åˆ¶ï¼‰
      const finalSystemPrompt = system_prompt || DEFAULT_SYSTEM_PROMPT;

      log.info('Request:', { model_id, messages_count: messages?.length, has_system_prompt: !!system_prompt });
    log.debug('system_prompt preview:', system_prompt ? `"${system_prompt.slice(0, 100)}..."` : 'using DEFAULT');
    log.debug('finalSystemPrompt:', finalSystemPrompt.length, 'chars, ~', Math.ceil(finalSystemPrompt.length / 4), 'tokens');

    // Token ä¼°ç®—å‡½æ•° (å­—ç¬¦æ•° / 4)
    const estimateTokens = (text) => Math.ceil((text || '').length / 4);

    // ========== Prompt Caching ç›¸å…³å¸¸é‡å’Œå‡½æ•° ==========
    const CACHE_MIN_TOKENS = 1024;  // æœ€å°ç¼“å­˜é˜ˆå€¼
    const MAX_CACHE_BREAKPOINTS = 4; // Claude æœ€å¤šæ”¯æŒ4ä¸ªç¼“å­˜æ–­ç‚¹

    // åˆ¤æ–­å†…å®¹æ˜¯å¦é€‚åˆç¼“å­˜
    const shouldEnableCache = (content, tokenCount) => {
      if (tokenCount < CACHE_MIN_TOKENS) return false;

      // æ£€æµ‹æ˜¯å¦åŒ…å«å¤§æ–‡æ¡£/RAG/ç»“æž„åŒ–æ•°æ®çš„ç‰¹å¾
      const hasStructuredData = /\|.*\|.*\|/m.test(content) || // CSV/è¡¨æ ¼
                                /```[\s\S]{500,}```/.test(content) || // å¤§ä»£ç å—
                                /<document>|<context>|<reference>/i.test(content); // RAGæ ‡è®°
      const hasRoleCard = /<character>|<persona>|<system_config>/i.test(content);

      return tokenCount >= CACHE_MIN_TOKENS || hasStructuredData || hasRoleCard;
    };

    // ========== API æ€§èƒ½ç›‘æŽ§å’Œæˆæœ¬ç»Ÿè®¡ ==========
    const getModelRates = (modelId) => {
      const lower = (modelId || '').toLowerCase();

      // Sonnet 4.5
      if (lower.includes('sonnet')) {
        return { input: 3.0, output: 15.0, cached: 0.3 }; // per 1M tokens
      }

      // Haiku 4.5
      if (lower.includes('haiku')) {
        return { input: 1.0, output: 5.0, cached: 0.1 }; // per 1M tokens
      }

      // Default to Sonnet pricing
      return { input: 3.0, output: 15.0, cached: 0.3 };
    };

    const logAPIPerformance = (modelId, usage, provider = 'anthropic') => {
      const rates = getModelRates(modelId);

      const inputTokens = usage.input_tokens || 0;
      const outputTokens = usage.output_tokens || 0;
      const cacheReadTokens = usage.cache_read_input_tokens || 0;
      const cacheCreationTokens = usage.cache_creation_input_tokens || 0;

      // è®¡ç®—æˆæœ¬ï¼ˆç¾Žå…ƒï¼‰
      const normalInputTokens = inputTokens - cacheReadTokens - cacheCreationTokens;
      const inputCost = (normalInputTokens / 1000000) * rates.input;
      const outputCost = (outputTokens / 1000000) * rates.output;
      const cacheCost = (cacheReadTokens / 1000000) * rates.cached;
      const cacheCreationCost = (cacheCreationTokens / 1000000) * rates.input * 1.25; // +25% for cache creation

      const totalCost = inputCost + outputCost + cacheCost + cacheCreationCost;

      // è®¡ç®—èŠ‚çœçš„æˆæœ¬ï¼ˆå¦‚æžœç¼“å­˜å‘½ä¸­ï¼‰
      const wouldBeCost = ((inputTokens - cacheCreationTokens) / 1000000) * rates.input + outputCost;
      const savedCost = wouldBeCost - totalCost;

      // ç¼“å­˜å‘½ä¸­çŽ‡
      const cacheHitRate = inputTokens > 0 ? (cacheReadTokens / inputTokens * 100).toFixed(1) : '0.0';

      // ä½¿ç”¨ info çº§åˆ«è¾“å‡ºå…³é”®æˆæœ¬ä¿¡æ¯
      log.info(`[API Monitor] ${modelId} | Input: ${inputTokens} | Output: ${outputTokens} | Cost: $${totalCost.toFixed(4)}`);

      // è¯¦ç»†æˆæœ¬ä¿¡æ¯ä½¿ç”¨ debug çº§åˆ«
      if (LOG_LEVEL >= 3) {
        console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
        console.log(`[API Monitor] ${modelId}`);
        console.log(`  ðŸ“Š Token Usage: Input ${inputTokens.toLocaleString()} | Output ${outputTokens.toLocaleString()}`);
        if (cacheReadTokens > 0) {
          console.log(`  ðŸ”„ Cache: Hit ${cacheReadTokens.toLocaleString()} tokens (${cacheHitRate}%) | Saved $${savedCost.toFixed(4)}`);
        }
        console.log(`  ðŸ’µ Total Cost: $${totalCost.toFixed(4)}`);
        console.log('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”');
      }

      return { totalCost, savedCost };
    };

    // æž„å»ºå¸¦ç¼“å­˜æ ‡è®°çš„æ¶ˆæ¯ï¼ˆç”¨äºŽ OpenRouter Anthropicï¼‰
    // ç®€åŒ–ç­–ç•¥ï¼šç¼“å­˜ç³»ç»Ÿæç¤ºè¯ + å€’æ•°ç¬¬4æ¡æ¶ˆæ¯ï¼ˆç¨³å®šè¾¹ç•Œï¼‰
    const buildCachedMessagesForOpenRouter = (msgs, sysPrompt) => {
      const result = [];
      let cacheEnabled = false;
      let cachedBlocksCount = 0;

      // è¾…åŠ©å‡½æ•°ï¼šæå–æ¶ˆæ¯çš„çº¯æ–‡æœ¬å†…å®¹
      const extractText = (content) => {
        if (!content) return '';
        if (Array.isArray(content)) {
          return content.map(block => block?.text || '').join('');
        }
        return typeof content === 'string' ? content : '';
      };

      // è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²ç»æœ‰ç¼“å­˜æŽ§åˆ¶
      const hasCacheControl = (content) => {
        if (!Array.isArray(content)) return false;
        return content.some(block => block?.cache_control);
      };

      // 1. ç³»ç»Ÿæç¤ºè¯ï¼šå¦‚æžœ >= 1024 tokensï¼Œå¯ç”¨ç¼“å­˜
      const sysTokens = estimateTokens(sysPrompt);
      const shouldCacheSystem = sysPrompt && sysTokens >= CACHE_MIN_TOKENS;

      if (sysPrompt) {
        if (shouldCacheSystem) {
          result.push({
            role: 'system',
            content: [{ type: 'text', text: sysPrompt, cache_control: { type: 'ephemeral' } }]
          });
          cacheEnabled = true;
          cachedBlocksCount++;
        } else {
          result.push({ role: 'system', content: sysPrompt });
        }
      }

      // 2. å¯¹è¯æ¶ˆæ¯ï¼šå¯¹å€’æ•°ç¬¬4æ¡æ¶ˆæ¯æ·»åŠ ç¼“å­˜æ ‡è®°ï¼ˆç¨³å®šéƒ¨åˆ†çš„è¾¹ç•Œï¼‰
      // æœ€æ–°3æ¡æ¶ˆæ¯å†…å®¹å˜åŒ–é¢‘ç¹ï¼Œä¸ç¼“å­˜
      const cachePoint = msgs.length - 4;

      msgs.forEach((m, idx) => {
        // å¦‚æžœæ¶ˆæ¯å·²ç»æœ‰ç¼“å­˜æŽ§åˆ¶ï¼Œä¿ç•™åŽŸæ ·
        if (hasCacheControl(m.content)) {
          result.push({ role: m.role, content: m.content });
          cacheEnabled = true;
          cachedBlocksCount++;
          return;
        }

        // èŽ·å–æ¶ˆæ¯æ–‡æœ¬å†…å®¹ï¼ˆå¤„ç†æ•°ç»„æ ¼å¼ï¼‰
        const textContent = extractText(m.content);

        // å€’æ•°ç¬¬4æ¡æ¶ˆæ¯ï¼šæ·»åŠ ç¼“å­˜æ ‡è®°
        if (idx === cachePoint && cachePoint >= 0) {
          result.push({
            role: m.role,
            content: [{ type: 'text', text: textContent, cache_control: { type: 'ephemeral' } }]
          });
          cacheEnabled = true;
          cachedBlocksCount++;
        } else {
          // å…¶ä»–æ¶ˆæ¯ï¼šä¸ç¼“å­˜ï¼Œä½†ç¡®ä¿æ ¼å¼æ­£ç¡®
          result.push({ role: m.role, content: textContent });
        }
      });

      return {
        messages: result,
        cacheEnabled,
        cachedBlocksCount
      };
    };

    // è¾…åŠ©å‡½æ•°ï¼šä»Žæ¶ˆæ¯å†…å®¹ä¸­æå–çº¯æ–‡æœ¬ï¼ˆå¤„ç†æ•°ç»„æ ¼å¼çš„ contentï¼‰
    const getMessageText = (content) => {
      if (!content) return '';
      if (Array.isArray(content)) {
        return content.map(block => block?.text || '').join('');
      }
      return typeof content === 'string' ? content : '';
    };

    // è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€» token æ•° - å®‰å…¨å¤„ç†æ•°ç»„æ ¼å¼çš„ content
    const calculateTotalTokens = (msgs, sysPrompt) => {
      let total = estimateTokens(sysPrompt);
      for (const m of msgs) {
        total += estimateTokens(getMessageText(m.content));
      }
      return total;
    };

    // æˆªæ–­åŽ†å²è®°å½•ï¼Œä¿æŒåœ¨å®‰å…¨é˜ˆå€¼å†…
    const truncateMessages = (msgs, sysPrompt, maxTokens) => {
      let truncatedMsgs = [...msgs];
      let totalTokens = calculateTotalTokens(truncatedMsgs, sysPrompt);
      
      while (totalTokens > maxTokens && truncatedMsgs.length > 2) {
        if (truncatedMsgs.length >= 2) {
          truncatedMsgs = truncatedMsgs.slice(2);
        } else {
          truncatedMsgs = truncatedMsgs.slice(1);
        }
        totalTokens = calculateTotalTokens(truncatedMsgs, sysPrompt);
      }
      
      return { truncatedMsgs, totalTokens };
    };

    // èŽ·å–æ¨¡åž‹é…ç½®
    const models = await base44.asServiceRole.entities.AIModel.filter({ id: model_id });
    const model = models[0];

    if (!model) {
      return Response.json({ error: 'Model not found' }, { status: 404 });
    }

    // ä½¿ç”¨æ¨¡åž‹é…ç½®çš„ input_limitï¼Œé»˜è®¤ 180000
    const inputLimit = model.input_limit || 180000;

    // æ‰§è¡Œæˆªæ–­
    const { truncatedMsgs: processedMessages, totalTokens } = truncateMessages(messages, finalSystemPrompt, inputLimit);

    // ä¼°ç®—è¾“å…¥tokens
    const estimatedInputTokens = calculateTotalTokens(processedMessages, finalSystemPrompt);
    
    log.debug('After truncation:', processedMessages.length, 'messages,', estimatedInputTokens, 'tokens');
    if (LOG_LEVEL >= 3) {
      processedMessages.forEach((m, i) => {
        const textContent = getMessageText(m.content);
        const isCached = Array.isArray(m.content);
        log.debug(`  [${i}] ${m.role}: ${textContent.slice(0, 50)}... (${Math.ceil(textContent.length / 4)} tokens, cached=${isCached})`);
      });
    }

    // åªæœ‰å½“provideræ˜¯builtinæ—¶æ‰ä½¿ç”¨å†…ç½®é›†æˆ
    if (model.provider === 'builtin') {
      const fullPrompt = processedMessages.map(m => {
        const textContent = getMessageText(m.content);
        if (m.role === 'user') return `ç”¨æˆ·: ${textContent}`;
        if (m.role === 'assistant') return `åŠ©æ‰‹: ${textContent}`;
        return textContent;
      }).join('\n\n');

      const finalPrompt = finalSystemPrompt
        ? `${finalSystemPrompt}\n\n${fullPrompt}\n\nè¯·æ ¹æ®ä¸Šè¿°å¯¹è¯åŽ†å²ï¼Œå›žå¤ç”¨æˆ·æœ€åŽçš„æ¶ˆæ¯ã€‚`
        : fullPrompt;

      // åªåœ¨æ˜Žç¡®è¦æ±‚æ—¶æ‰è”ç½‘ï¼Œä¸è‡ªåŠ¨ä½¿ç”¨æ¨¡åž‹é…ç½®
      const shouldUseWebSearch = force_web_search === true;
      log.debug('Using web search:', shouldUseWebSearch);

      const result = await base44.integrations.Core.InvokeLLM({
        prompt: finalPrompt,
        add_context_from_internet: shouldUseWebSearch
      });

      // ä¼°ç®—è¾“å‡ºtokens
      const estimatedOutputTokens = estimateTokens(result);
      
      // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†ï¼ˆç²¾ç¡®å°æ•°ï¼‰
      const inputCredits = estimatedInputTokens / 1000;
      const outputCredits = estimatedOutputTokens / 200;

      return Response.json({
        response: result,
        input_tokens: estimatedInputTokens,
        output_tokens: estimatedOutputTokens,
        input_credits: inputCredits,
        output_credits: outputCredits,
        credits_used: inputCredits + outputCredits,
        web_search_enabled: force_web_search === true
      });
    }

    if (!model.api_key) {
      return Response.json({ error: 'API key not configured for this model' }, { status: 400 });
    }

    const provider = model.provider;
    let responseText;
    let actualInputTokens = estimatedInputTokens;
    let actualOutputTokens = 0;

    // æž„å»ºæ¶ˆæ¯åˆ—è¡¨
    let formattedMessages = processedMessages.map(m => ({
      role: m.role,
      content: m.content
    }));

    const useOpenAIFormat = model.api_endpoint && model.api_endpoint.includes('/chat/completions');

    // CRITICAL: åªæœ‰å½“ finalSystemPrompt æœ‰å®žé™…å†…å®¹æ—¶æ‰æ·»åŠ 
    const hasValidSystemPrompt = finalSystemPrompt && finalSystemPrompt.trim().length > 0;
    log.debug('hasValidSystemPrompt:', hasValidSystemPrompt);

    if (hasValidSystemPrompt && !useOpenAIFormat && provider !== 'anthropic') {
      log.debug('Adding system prompt to messages, length:', finalSystemPrompt.length);
      formattedMessages.unshift({ role: 'system', content: finalSystemPrompt });
    }

    // å¦‚æžœæœ‰å›¾ç‰‡æ–‡ä»¶ï¼Œå°†æœ€åŽä¸€æ¡ç”¨æˆ·æ¶ˆæ¯è½¬æ¢ä¸ºå¤šæ¨¡æ€æ ¼å¼
    if (image_files && image_files.length > 0) {
      const lastUserMsgIdx = formattedMessages.length - 1;
      if (formattedMessages[lastUserMsgIdx]?.role === 'user') {
        // å®‰å…¨æå–æ–‡æœ¬å†…å®¹ï¼ˆå¤„ç†æ•°ç»„æ ¼å¼ï¼‰
        const textContent = getMessageText(formattedMessages[lastUserMsgIdx].content);
        const contentArray = [];

        // æ·»åŠ å›¾ç‰‡
        image_files.forEach(img => {
          contentArray.push({
            type: 'image',
            source: {
              type: 'base64',
              media_type: img.media_type,
              data: img.base64
            }
          });
        });

        // æ·»åŠ æ–‡æœ¬
        contentArray.push({
          type: 'text',
          text: textContent
        });

        formattedMessages[lastUserMsgIdx] = {
          role: 'user',
          content: contentArray
        };
      }
    }

    // è®°å½•æœ€ç»ˆå‘é€åˆ°APIçš„æ¶ˆæ¯
    log.debug('Final messages to API:', formattedMessages.length, 'messages, has_images:', !!(image_files && image_files.length > 0));

    if (useOpenAIFormat || provider === 'openai' || provider === 'custom') {
      const endpoint = model.api_endpoint || 'https://api.openai.com/v1/chat/completions';
      const isOpenRouter = model.api_endpoint && model.api_endpoint.includes('openrouter.ai');

      // æž„å»ºè¯·æ±‚ä½“
      const requestBody = {
        model: model.model_id,
        messages: formattedMessages,
        max_tokens: model.max_tokens || 4096
      };

      // å¦‚æžœæ˜¯OpenRouterä¸”æ˜Žç¡®è¦æ±‚å¯ç”¨è”ç½‘æœç´¢ï¼Œæ·»åŠ pluginså‚æ•°
      if (isOpenRouter && force_web_search === true) {
        requestBody.plugins = [
          {
            id: 'web',
            max_results: 5
          }
        ];
      }

      log.info('OpenAI/Custom API:', endpoint, '| OpenRouter:', isOpenRouter);

      const res = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${model.api_key}`
        },
        body: JSON.stringify(requestBody)
      });

      if (!res.ok) {
        const error = await res.text();
        return Response.json({ error: `API error: ${error}` }, { status: res.status });
      }

      const data = await res.json();
      responseText = data.choices[0].message.content;
      
      // ä½¿ç”¨APIè¿”å›žçš„çœŸå®žtokenæ•°
      if (data.usage) {
        actualInputTokens = data.usage.prompt_tokens || estimatedInputTokens;
        actualOutputTokens = data.usage.completion_tokens || estimateTokens(responseText);
      } else {
        actualOutputTokens = estimateTokens(responseText);
      }

      // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†ï¼ˆç²¾ç¡®å°æ•°ï¼‰
      const inputCredits = actualInputTokens / 1000;
      const outputCredits = actualOutputTokens / 200;

      return Response.json({
        response: responseText,
        input_tokens: actualInputTokens,
        output_tokens: actualOutputTokens,
        input_credits: inputCredits,
        output_credits: outputCredits,
        credits_used: inputCredits + outputCredits,
        model: data.model || null,
        usage: data.usage || null,
        web_search_enabled: force_web_search === true
      });

    } else if (provider === 'anthropic') {
      const endpoint = model.api_endpoint || 'https://api.anthropic.com/v1/messages';
      const isOfficialApi = !model.api_endpoint || model.api_endpoint.includes('anthropic.com');
      const isOpenRouter = model.api_endpoint && model.api_endpoint.includes('openrouter.ai');

      const headers = {
        'Content-Type': 'application/json'
      };

      if (isOfficialApi) {
        headers['x-api-key'] = model.api_key;
        headers['anthropic-version'] = '2023-06-01';
      } else {
        headers['Authorization'] = `Bearer ${model.api_key}`;
      }

      // ========== OpenRouter Anthropic æ¨¡åž‹è°ƒç”¨ï¼ˆæ”¯æŒ Prompt Cachingï¼‰==========
      if (isOpenRouter) {
        // æž„å»ºå¸¦ç¼“å­˜æ ‡è®°çš„æ¶ˆæ¯
        const { messages: cachedMessages, cacheEnabled, cachedBlocksCount } =
          buildCachedMessagesForOpenRouter(processedMessages, finalSystemPrompt);

        const requestBody = {
          model: model.model_id,
          messages: cachedMessages,
          max_tokens: model.max_tokens || 4096
        };

        // å¦‚æžœæ˜Žç¡®è¦æ±‚å¯ç”¨è”ç½‘æœç´¢ï¼Œæ·»åŠ pluginså‚æ•°
        if (force_web_search === true) {
          requestBody.plugins = [{ id: 'web', max_results: 5 }];
        }

        log.info('Anthropic API (OpenRouter) | Cache:', cacheEnabled);

        const res = await fetch(endpoint, {
          method: 'POST',
          headers,
          body: JSON.stringify(requestBody)
        });

        if (!res.ok) {
          const error = await res.text();
          return Response.json({ error: `API error: ${error}` }, { status: res.status });
        }

        const data = await res.json();
        responseText = data.choices?.[0]?.message?.content || data.content?.[0]?.text;

        // è§£æž token ä½¿ç”¨æƒ…å†µï¼ˆåŒ…æ‹¬ç¼“å­˜ç»Ÿè®¡ï¼‰
        let cachedTokens = 0;
        
        if (data.usage) {
          actualInputTokens = data.usage.prompt_tokens || data.usage.input_tokens || estimatedInputTokens;
          actualOutputTokens = data.usage.completion_tokens || data.usage.output_tokens || estimateTokens(responseText);
          
          // OpenRouter è¿”å›žçš„ç¼“å­˜ç»Ÿè®¡
          cachedTokens = data.usage.prompt_tokens_details?.cached_tokens || 
                         data.usage.cache_read_input_tokens || 0;
        } else {
          actualOutputTokens = estimateTokens(responseText);
        }

        // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†ï¼ˆç¼“å­˜å‘½ä¸­90%æŠ˜æ‰£ï¼‰
        const uncachedInputTokens = actualInputTokens - cachedTokens;
        const cachedInputCredits = (cachedTokens / 1000) * 0.1; // ç¼“å­˜å‘½ä¸­90%æŠ˜æ‰£
        const uncachedInputCredits = uncachedInputTokens / 1000;
        const inputCredits = cachedInputCredits + uncachedInputCredits;
        const outputCredits = actualOutputTokens / 200;

        // è®¡ç®—ç¼“å­˜èŠ‚çœçš„ç§¯åˆ†
        const creditsSaved = cachedTokens > 0 ? (cachedTokens / 1000) * 0.9 : 0;

        // ä½¿ç”¨æ–°çš„æ€§èƒ½ç›‘æŽ§æ—¥å¿—
        if (data.usage) {
          logAPIPerformance(model.model_id, {
            input_tokens: actualInputTokens,
            output_tokens: actualOutputTokens,
            cache_read_input_tokens: cachedTokens,
            cache_creation_input_tokens: 0 // OpenRouter doesn't report cache creation separately
          }, 'openrouter');
        }

        return Response.json({
          response: responseText,
          input_tokens: actualInputTokens,
          output_tokens: actualOutputTokens,
          input_credits: inputCredits,
          output_credits: outputCredits,
          credits_used: inputCredits + outputCredits,
          usage: data.usage || null,
          web_search_enabled: force_web_search === true,
          // ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
          cache_enabled: cacheEnabled,
          cached_blocks_count: cachedBlocksCount,
          cached_tokens: cachedTokens,
          cache_hit_rate: actualInputTokens > 0 ? (cachedTokens / actualInputTokens * 100).toFixed(1) + '%' : '0%',
          credits_saved_by_cache: creditsSaved
        });
      }

      // ========== å®˜æ–¹ Anthropic API ==========
      const anthropicMessages = formattedMessages.filter(m => m.role !== 'system');

      // æž„å»ºå¸¦ç¼“å­˜çš„ system å‚æ•°
      const systemTokens = estimateTokens(finalSystemPrompt);
      const shouldCacheSystem = systemTokens >= CACHE_MIN_TOKENS;

      const requestBody = {
        model: model.model_id,
        max_tokens: model.max_tokens || 4096,
        messages: anthropicMessages
      };

      // å¦‚æžœç³»ç»Ÿæç¤ºè¯è¶³å¤Ÿé•¿ï¼Œå¯ç”¨ Prompt Caching
      if (finalSystemPrompt) {
        if (shouldCacheSystem) {
          requestBody.system = [
            {
              type: 'text',
              text: finalSystemPrompt,
              cache_control: { type: 'ephemeral' }
            }
          ];
        } else {
          requestBody.system = finalSystemPrompt;
        }
      }

      log.info('Anthropic API (Official) | Model:', model.model_id, '| Cache:', shouldCacheSystem);

      const res = await fetch(endpoint, {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody)
      });

      if (!res.ok) {
        const error = await res.text();
        return Response.json({ error: `API error: ${error}` }, { status: res.status });
      }

      const data = await res.json();
      responseText = data.content[0].text;

      // Anthropic è¿”å›ž usageï¼ˆåŒ…å«ç¼“å­˜ç»Ÿè®¡ï¼‰
      let cachedTokens = 0;
      let cacheCreationTokens = 0;

      if (data.usage) {
        actualInputTokens = data.usage.input_tokens || estimatedInputTokens;
        actualOutputTokens = data.usage.output_tokens || estimateTokens(responseText);

        // è¯»å–ç¼“å­˜ç»Ÿè®¡
        cachedTokens = data.usage.cache_read_input_tokens || 0;
        cacheCreationTokens = data.usage.cache_creation_input_tokens || 0;
      } else {
        actualOutputTokens = estimateTokens(responseText);
      }

      // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†ï¼ˆç¼“å­˜å‘½ä¸­90%æŠ˜æ‰£ï¼‰
      const uncachedInputTokens = actualInputTokens - cachedTokens - cacheCreationTokens;
      const cachedInputCredits = (cachedTokens / 1000) * 0.1; // ç¼“å­˜å‘½ä¸­90%æŠ˜æ‰£
      const cacheCreationCredits = (cacheCreationTokens / 1000) * 1.25; // ç¼“å­˜å†™å…¥25%æº¢ä»·
      const uncachedInputCredits = uncachedInputTokens / 1000;
      const inputCredits = cachedInputCredits + cacheCreationCredits + uncachedInputCredits;
      const outputCredits = actualOutputTokens / 200;

      // è®¡ç®—ç¼“å­˜èŠ‚çœçš„ç§¯åˆ†
      const creditsSaved = cachedTokens > 0 ? (cachedTokens / 1000) * 0.9 : 0;

      // ä½¿ç”¨æ–°çš„æ€§èƒ½ç›‘æŽ§æ—¥å¿—
      logAPIPerformance(model.model_id, data.usage, 'anthropic-official');

      return Response.json({
        response: responseText,
        input_tokens: actualInputTokens,
        output_tokens: actualOutputTokens,
        input_credits: inputCredits,
        output_credits: outputCredits,
        credits_used: inputCredits + outputCredits,
        usage: data.usage || null,
        web_search_enabled: false,
        // ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        cache_enabled: shouldCacheSystem,
        cached_tokens: cachedTokens,
        cache_creation_tokens: cacheCreationTokens,
        cache_hit_rate: actualInputTokens > 0 ? (cachedTokens / actualInputTokens * 100).toFixed(1) + '%' : '0%',
        credits_saved_by_cache: creditsSaved
      });

    } else if (provider === 'google') {
      const endpoint = model.api_endpoint || `https://generativelanguage.googleapis.com/v1beta/models/${model.model_id}:generateContent?key=${model.api_key}`;

      const geminiContents = formattedMessages
        .filter(m => m.role !== 'system')
        .map(m => ({
          role: m.role === 'assistant' ? 'model' : 'user',
          parts: [{ text: getMessageText(m.content) }]
        }));

      const requestBody = {
        contents: geminiContents,
        generationConfig: {
          maxOutputTokens: model.max_tokens || 4096
        }
      };

      if (finalSystemPrompt) {
        requestBody.systemInstruction = { parts: [{ text: finalSystemPrompt }] };
      }

      log.info('Google Gemini API | Model:', model.model_id);

      const res = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestBody)
      });

      if (!res.ok) {
        const error = await res.text();
        return Response.json({ error: `API error: ${error}` }, { status: res.status });
      }

      const data = await res.json();
      responseText = data.candidates[0].content.parts[0].text;
      
      // Geminiè¿”å›žusageMetadata
      if (data.usageMetadata) {
        actualInputTokens = data.usageMetadata.promptTokenCount || estimatedInputTokens;
        actualOutputTokens = data.usageMetadata.candidatesTokenCount || estimateTokens(responseText);
      } else {
        actualOutputTokens = estimateTokens(responseText);
      }

      // æ–°è®¡è´¹è§„åˆ™ï¼šè¾“å…¥1000tokens=1ç§¯åˆ†ï¼Œè¾“å‡º200tokens=1ç§¯åˆ†
      const inputCredits = actualInputTokens / 1000;
      const outputCredits = actualOutputTokens / 200;

      return Response.json({
        response: responseText,
        input_tokens: actualInputTokens,
        output_tokens: actualOutputTokens,
        input_credits: inputCredits,
        output_credits: outputCredits,
        credits_used: inputCredits + outputCredits,
        modelVersion: data.modelVersion || null,
        usageMetadata: data.usageMetadata || null,
        web_search_enabled: false
      });

      } else {
      return Response.json({ error: `Unsupported provider: ${provider}` }, { status: 400 });
    }

  } catch (error) {
    return Response.json({ error: error.message }, { status: 500 });
  }
});